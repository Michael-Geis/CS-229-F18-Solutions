{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS2-2: Model Calibration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We say that a logistic regression model $h_\\theta(x) = (1 + \\exp(-x\\cdot\\theta))^{-1}$ is well calibrated on the interval $(a,b)$ if the following holds. Let $S_{a,b} = \\{i : h_\\theta(x^i) \\in (a,b) \\}$ and $S^+_{a,b} = \\{i \\in S ~|~ y^i = 1 \\}$ be the set of indices of positive training examples in $S$. Then\n",
    "\n",
    "$$\\frac{1}{|S_{a,b}|} \\sum_{i \\in S} h_\\theta(x^i) = \\frac{|S^+_{a,b}|}{|S_{a,b}|}$$\n",
    "\n",
    "That is, the average of all predicted probabilities in the range $(a,b)$ is about the fraction of positive examples among such training examples.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A)\n",
    "#### Show that the following logistic regression model is well-calibrated on $(0,1)$:\n",
    "\n",
    "#### Let $\\{x^i,y^i\\}$ be the set of training data with $x^i \\in \\mathbb{R}^{n+1}$, $x^i_0 = 1$ for all $i$ (i.e. we include a bias term). Assume that $\\theta$ are the parameters corresponding to maximum likelihood estimation.\n",
    "\n",
    "At this particular value of $\\theta$, the gradient of the log likelihood vanishes, i.e.\n",
    "\n",
    "$$\\nabla \\ell(\\theta) = \\sum_{i=1}^m (y^i - h_\\theta(x^i))x^i = 0$$\n",
    "\n",
    "But then, the $0^{th}$ component of this vector vanishes, so\n",
    "\n",
    "$$\\sum_{i=1}^m y^i - h_\\theta(x^i) = 1$$\n",
    "\n",
    "Since all $m$ examples satisfy $h_\\theta(x^i) \\in (0,1)$, this means that\n",
    "\n",
    "$$\\frac{1}{m}\\sum_{i=1}^m h_\\theta(x^i) = \\frac{1}{m}\\sum_{i=1}^m y^i$$\n",
    "\n",
    "which is the well-calibrated statement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B)\n",
    "#### If a binary classification model is perfectly calibrated i.e. well-calibrated on every interval $(a,b)$, does it necessarily hold that the model achieves perfect accuracy? Is the converse true?\n",
    "\n",
    "Perfect calibration does not imply perfect accuracy. First, note that for a fixed $\\theta$, the map $x \\mapsto h_\\theta(x)$ is not injective, unless $x^i \\in \\mathbb{R}$ and $\\theta \\neq 0$. So it is possible that we have 2 distinct $x^i,x^j$ with $h_\\theta(x^i) = h_\\theta(x^j) = p$ and no others. Then choose $a < p < b$ so that no other datapoint satisfies $h_\\theta(x) \\in (a,b)$. By the calibration propery, we have \n",
    "\n",
    "$$h_\\theta(x^i) = p = \\frac{1}{2}(y^i + y^j)$$\n",
    "\n",
    "but the right hand side is either $0$, $1$, or $\\frac{1}{2}$. If it is $\\frac{1}{2}$, it is impossible for the model to correctly predict both points.\n",
    "\n",
    "OTOH, perfect prediction does not imply perfect calibration. Perfect prediction means that the model satisfies \n",
    "$h_\\theta(x^i) > 1/2$ if and only if $y^i = 1$. If we choose $a = \\frac{1}{2}$ , $b = 1$, then $S_{a,b}$ is the set of all data points which are predicted to be positive by the model. Because it is perfectly accurately, all of these are true positive examples, so the fraction of positive examples within this set is one. If the model were well calibrated on this interval, then\n",
    "\n",
    "$$\\frac{1}{|\\{y^i = 1\\}|}\\sum_{y^i = 1} h_\\theta(x^i) = 1$$\n",
    "\n",
    "but this is impossible unless $h_\\theta(x^i) = 1$ for all positive examples.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C)\n",
    "#### Describe what effect adding an $L^2$ regularization term to the model in part (a) has on model calibration.\n",
    "\n",
    "The gradient of the objective function with regularization is \n",
    "\n",
    "$$\\nabla\\left(\\ell - \\frac{\\lambda}{2}||\\theta||^2\\right) = \\sum_{i=1}^m (y^i - h_\\theta(x^i))x^i - \\lambda\\theta $$\n",
    "\n",
    "But the additional term destroys the property that guaranteed calibration on $(0,1)$ unless $\\theta_0 = 0$. But in this case, there is no longer a bias term in the model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
