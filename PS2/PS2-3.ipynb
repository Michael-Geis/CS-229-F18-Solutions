{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS2-1: Bayesian Interpretation of Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction, notation, and terminology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Bayesian framework\n",
    "\n",
    "We have $m$ points of data $X = \\{x_1 , \\dots , x_m\\}$ which are *observations*.\n",
    "\n",
    "Assume:\n",
    "\n",
    "1. There is a family of probability distributions $p_\\theta(x)$ from which the data is drawn independently\n",
    "1. The parameters are themselves a random variable distributed according to a distribution $p(\\theta)$.\n",
    "\n",
    "The distribution of the random variable from which the samples are drawn is given by\n",
    "\n",
    "$$p(x) = \\int p(x | \\theta) p(\\theta) \\, d\\theta$$\n",
    "\n",
    "After seeing the observations $X$, we update our belief of the distribution of $\\theta$. The **posterior\n",
    "distribution** of the parameters is\n",
    "\n",
    "$$p(\\theta|X).$$\n",
    "\n",
    "We then use this to update our model for the distribution of the random variabel $x$. The **posterior predictive distribution** of the random variable $x$ is\n",
    "\n",
    "$$p(x|X) = \\int p(x|\\theta)p(\\theta|X) \\, d\\theta.$$\n",
    "\n",
    "Notice that this follows from the chain rule of probability because of the assumption that the samples are independent given $\\theta$, i.e. we assume that $p(x | X , \\theta) = p(x | \\theta)$, so\n",
    "\n",
    "$$p(x|X) = \\int p(x|X,\\theta)p(\\theta|X)\\,d\\theta = \\int p(x|\\theta)p(\\theta|X) \\, d\\theta$$\n",
    "\n",
    "Summary of terms:\n",
    "\n",
    "- **Model**: The joint distribution of all quantities, observed and unobserved, $p(x,\\theta)$.\n",
    "- **Prior distribution**: The marginal distribution of the (unobserved) parameters $\\theta$, $p(\\theta)$.\n",
    "- **Sampling distribution** The distribution the samples are drawn from $p(x) = \\int p(x|\\theta)p(\\theta)\\,d\\theta$.\n",
    "- **Posterior distribution**: The distribution of the unobserved parameters *after* seeing the observed data, $p(\\theta|X)$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two examples of Bayesian analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Modeling a coin toss with a uniform prior\n",
    "\n",
    "Consider a random a repeated coin toss where samples are drawn iid from a Bernoulli distribution $p(H) = \\phi$, $p(T) = 1 - \\phi$. Suppose we have flipped a coin 5 times and gotten $X = \\{H,H,T,H,T\\}$ as results. Our prior assumption is that the Bernoulli parameter $\\phi$ is actually uniform in $[0,1]$, so that $\\phi$ has the distribution $p(\\phi) = 1$. Note that before seeing any evidence, the sampling distribution is\n",
    "\n",
    "$$p(H) = \\int_0^1 p(H|\\phi) \\, d\\phi = \\frac{1}{2} = P(T)$$\n",
    "\n",
    "Now by Bayes' rule, the posterior distribution of $\\phi$ is given by\n",
    "\n",
    "$$p(\\phi | X) \\sim p(X | \\phi) p(\\phi) = c\\phi^3(1-\\phi)^2$$\n",
    "\n",
    "where $c^{-1} = \\int_0^1 \\phi^3(1-\\phi^2) \\,d\\phi = 1/60$. Now we can calculate the posterior predictive distribution;\n",
    "\n",
    "$$p(H|X) = \\int_0^1 p(H | \\phi) p(\\phi | X) = 60 \\int_0^1 \\phi^4(1-\\phi)^2 \\,d\\phi \\approx 0.57$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2. A mixture of Gaussians\n",
    "\n",
    "We consider drawing independent random samples from finitely many Gaussian distributions $q_j \\sim N(\\mu_j,\\Sigma_j)$, $j=1,\\dots,k$.\n",
    "Let $L$ be a random variable taking values in $\\{1,...,k\\}$ which labels the Gaussian a sample comes from. We might take the prior distribution of $L$ to be $p(L=j) = 1/k$, i.e. each Gaussian is equally likely to be drawn from. \n",
    "\n",
    "Given $m$ observations of the data, $X =\\{x_1,\\dots,x_m\\}$, we can then use bayesian inference to calculate the posterior predictive distribution\n",
    "of the samples after seeing the data $X$. The posterior distribution of the labels is\n",
    "\n",
    "\n",
    "$$p_{L|x=X}(j) = \\frac{p_{x|L=j}(X)p_L(j)}{\\sum_{j=1}^k p_{x|L=j}(X)p_L(j)}$$\n",
    "\n",
    "and thus the posterior predictive distribution is \n",
    "\n",
    "\n",
    "$$p_{x|X}(z) = \\sum_{j=1}^k p_{x|L=j}(z)p_{L|x=X}(j) = \\sum_{j=1}^k p_{x|L=j}(z) \\frac{p_{x|L=j}(X)p_L(j)}{\\sum_{j=1}^k p_{x|L=j}(X)p_L(j)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Estimation\n",
    "\n",
    "#### An alternative to the posterior predictive distribution\n",
    "\n",
    "Often times it is too difficult to store the entire posterior distribution and make predictions with the posterior sampling distribution. Instead, there is a common approximation used. Instead of carrying over the entire posterior distribution of parameters, we just condense that into a single 'most likely' choice of parameter, $\\theta_0$, and then suppose our sampling distribution is \n",
    "\n",
    "$$p_{x|\\theta = \\theta_0}$$\n",
    "\n",
    "The most common choice is to take the mode of the posterior distribution, $\\theta_0 = \\argmax_\\theta p(\\theta | X)$. This is called **maximum a posteriori estimation** and we usually write the mode as $\\theta_{MAP}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem statement\n",
    "\n",
    "We consider MAP estimation as applied to the obversation of labeled data. That is, assume our labeled data $(x^i,y^i)$ is drawn independently from a joint distribution. We model the conditional distributions $p(y|x,\\theta)$ by varying over some space of parameters $\\theta$. We equip the parameters $\\theta$ with a prior distribution $p(\\theta)$. \n",
    "\n",
    "\n",
    "We will assume that $p(\\theta) = p(\\theta|x)$. That is, the parameters are independent of the inputs. This is reasonable since we are modeling the conditional distributions $p(y|x)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A)\n",
    "#### With the assumption of $p(\\theta|x) = p(\\theta)$, show that\n",
    "\n",
    "#### $$\\theta_{\\text{MAP}} = \\argmax_\\theta p(y|x,\\theta)p(\\theta)$$\n",
    "\n",
    "The posterior distribution is \n",
    "\n",
    "$$p(\\theta|x,y) = \\frac{p(\\theta,x,y)}{p(x,y)} = \\frac{p(y|x,\\theta)p(x,\\theta)}{p(x,y)} = \\frac{p(y|x,\\theta)p(\\theta|x)p(x)}{p(x,y)}$$\n",
    "\n",
    "Applying the assumption to the equality we get\n",
    "\n",
    "$$p(\\theta|x,y) = \\frac{p(y|x,\\theta)p(\\theta)}{p(y|x)}$$\n",
    "\n",
    "from which the claim follows by taking the argmax in $\\theta$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B)\n",
    "#### Suppose that $\\theta \\sim N(0,\\eta^2\\text{Id})$. Show that MAP estimation reduces to MLE with an $L^2$ regularization term.\n",
    "\n",
    "From part (A), \n",
    "\n",
    "$$\\theta_{MAP} = \\argmax p(y|x,\\theta)p(\\theta) = \\argmax_\\theta \\left( \\log p(y|x,\\theta) + \\log p(\\theta) \\right)$$\n",
    "\n",
    "And \n",
    "\n",
    "$$p(\\theta) = (2\\pi)^{-\\frac{n}{2}}\\eta^{-n} \\exp \\left(- \\frac{||\\theta||^2}{2\\eta^2}\\right)$$\n",
    "\n",
    "so after taking logs and discarding the term that doesn't depend on $\\theta$,\n",
    "\n",
    "$$\\theta_{MAP} = \\argmax_\\theta \\left( \\log p(y|x,\\theta) + \\frac{1}{2\\eta^2}||\\theta||^2 \\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) \n",
    "#### Consider a concrete version of the setup in part (B), a linear regression model.\n",
    "\n",
    "#### Assume that\n",
    "\n",
    "1. $y = \\theta \\cdot x + \\epsilon$ where $\\epsilon \\sim N(0,\\sigma^2)$.\n",
    "1. Assume a Gaussian prior $\\theta \\sim N(0,\\eta^2\\text{Id})$. \n",
    "\n",
    "#### Find a closed form expression for $\\theta_{MAP}$.\n",
    "\n",
    "Let $X$ be the matrix of sample inputs where each input is a row, and $Y$ the vector of sample labels. In this case the conditional distribution of the labels is $p(y^i|x^i,\\theta) \\sim N(x^i\\cdot\\theta,\\sigma^2)$. So if we write this out and take logs, discarding the term that doesn't depend on $\\theta$,\n",
    "\n",
    "$$\\theta_{MAP} = \\argmax_\\theta \\left(\\frac{1}{2\\sigma^2} \\sum_{i=1}^m|y^i - x^i\\cdot\\theta|^2  + \\frac{1}{2\\eta^2}||\\theta||^2\\right)$$\n",
    "\n",
    "The right hand side can be written as \n",
    "\n",
    "$$\\frac{1}{2\\sigma^2} ||Y - X\\theta||^2 + \\frac{1}{2\\eta^2}||\\theta||^2$$\n",
    "\n",
    "This is stationary in $\\theta$ exactly when \n",
    "\n",
    "$$(Y - X\\theta)^TX + \\frac{\\sigma^2}{\\eta^2}\\theta^T = 0 $$\n",
    "\n",
    "or $$Y^TX - \\theta^T(X^TX + \\frac{\\sigma}{\\eta}\\text{I}) = 0$$\n",
    "\n",
    "Which means \n",
    "\n",
    "$$\\theta = (X^TX + \\frac{\\sigma^2}{\\eta^2}\\text{I})^{-1}X^TY.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D)\n",
    "#### Consider the same linear regression problem but now suppose that the prior distribution of $\\theta$ is Laplace; that is\n",
    "\n",
    "$$p(\\theta) = \\frac{1}{2b} \\exp \\left(-\\frac{||\\theta||}{b}\\right)$$\n",
    "\n",
    "Calculate the quantity to maximize to obtain $\\theta_{MAP}$.\n",
    "\n",
    "Repeat the analysis from the last part. The objective function to maximize is \n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{\\sigma^2}||X\\theta - Y||^2 - \\frac{1}{b}||\\theta||$$\n",
    "\n",
    "which is the standard mean-square error term plus $L^1$ regularization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
