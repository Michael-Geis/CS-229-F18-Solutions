{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS2-5: Kernelizing the Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Consider a binary classification problem with labels $y \\in \\{0,1\\}$. We will train a perceptron classifier. That is, the model predictions will be of the form \n",
    "\n",
    "$$h_\\theta(x) = \\text{sign}\\,(\\theta \\cdot x)$$\n",
    "\n",
    "Where $\\text{sign}\\, t = 1$ if $t \\geq 0$, and 0 otherwise. We will optimize the value of $\\theta$ by using stochastic gradient descent, but only looping through the entire dataset once. That is, for our training set $\\{x^i,y^i\\}$, we will initialize $\\theta_0 = 0$ and then update $\\theta$ according to the rule:\n",
    "\n",
    "$$\\theta_{i+1} = \\theta_i + \\alpha(y^{i+1} - h_{\\theta_i}(x^{i+1}))x^{i+1}$$\n",
    "\n",
    "Therefore, $\\theta_i$ is the value of $\\theta$ once the algorithm has seen $i$ training examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A)\n",
    "#### Describe a strategy for implementing this algorithm using the Kernel trick. That is, suppose we have a high dimensional embedding $x \\mapsto \\phi(x)$ and we would like to implement the algorithm in the image of $\\phi$, i.e. on the training set $\\{\\phi(x^i),y^i\\}$.\n",
    "\n",
    "#### Suppose the kernel $K(x,y)$ is defined by $K(x,y) = \\phi(x) \\cdot \\phi(y)$. \n",
    "\n",
    "1. How will the (high dimensional) parameter vector $\\theta_i$ be represented?\n",
    "1. How will the prediction $h_{\\theta_i}(x^{i+1}) = \\text{sign}\\, (\\theta_i \\cdot \\phi(x^{i+1}))$ be calculated efficiently?\n",
    "1. How will the update rule described in the previous block be modified to give the correct update rule in the high dimensional embedding space?\n",
    "\n",
    "The crucial observation is that after each update step, the parameter vector $\\theta_{i+1}$ is determined by adding a scalar multiple of the vector $\\phi(x^{i+1})$ to the previous parameter vector $\\theta_i$. Therefore, after training, the parameter vector will simply be a linear combination of the images of the training examples:\n",
    "\n",
    "$$\\theta_{m} = \\sum_{i=1}^m \\beta_i \\phi(x^i)$$\n",
    "\n",
    "Therefore, we need only store the parameter vector $\\theta_m$ as the vector of coefficients $\\beta_i$. More precisely for each $i \\in \\{0,\\dots,m-1\\}$,\n",
    "\n",
    "$$\\theta_{i+1} = \\sum_{j=1}^{i+1} \\beta_j \\phi(x^{j})$$\n",
    "\n",
    "and therefore \n",
    "\n",
    "$$h_{\\theta_i}(x^{i+1}) = \\text{sign} \\,\\left( \\sum_{j=1}^i \\beta_j \\phi(x^j) \\cdot \\phi(x^{i+1})  \\right) = \\text{sign} \\left( \\sum_{j=1}^i \\beta_j K(x^j,x^{i+1}) \\right)$$\n",
    "\n",
    "According to the update rule, the coefficient $\\beta_{i+1}$ is given by\n",
    "\n",
    "$$\\beta_{i+1} = \\alpha(y^{i+1} - h_{\\theta_{i}}(x^{i+1})) = \\alpha\\left( y^{i+1} - \\text{sign}\\left( \\sum_{j=1}^i \\beta_j K(x^j,x^{i+1})\\right)\\right)$$\n",
    "\n",
    "Finally, to make a prediction on a new input $x$ after training, the model will output\n",
    "\n",
    "$$h_{\\theta_m}(x) = \\text{sign} \\left( \\sum_{j=1}^m \\beta_j K(x_j,x) \\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C)\n",
    "#### The perceptron algorithm is coded in `/src/p05_percept.py`. Its predictions for the data/ds_test.csv dataset are saved in the output folder using both a dot product kernel and a radial bias kernel. One of these performs much better than the other, why is that?\n",
    "\n",
    "The radial bias kernel outputs a much better decision boundary. Using the dot product kernel corresponds to a feature map $\\phi(x) = x$, or the ordinary perceptron which attempts to find a linear decision boundary. Since the data is not linearly separable, this performs much worse than the RBF kernel."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
