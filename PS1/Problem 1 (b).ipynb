{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbee2e65",
   "metadata": {},
   "source": [
    "## Overview of the logistic regression model\n",
    "\n",
    "Recall the definition of the logistic function:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + \\exp ( - z )} $$\n",
    "\n",
    "The logistic regression algorithm seeks to predict a binary label $y \\in \\{ 0 , 1 \\}$ for a given data point $x \\in \\mathbb{R}^n$. Given a training set of $m$ labeled data points $(x^i , y^i)$, we let $X$ be the $m \\times n$ matrix whose $i^{th}$ row is $x^i$ and $y \\in \\{0,1\\}^m$ be the vectorized list of labels for this data. For a parameter $\\theta \\in \\mathbb{R}^n$ to be determined, we suppose that the probability that $x$ will have the label $y = 1$ is given by the logistic function precomposed with a linear map in $x$, i.e. \n",
    "\n",
    "$$P( y = 1 | x ; \\theta) = h(\\theta,x) = g(\\theta \\cdot x)$$\n",
    "\n",
    "where $\\theta \\cdot x$ is the usual Euclidean inner product. We will choose $\\theta$ according to maximum likelihood estimation, which means we seek to maximize likelihood of theta, i.e. the conditional probability, \n",
    "\n",
    "$$L(\\theta) = P(y | X ; \\theta) = \\Pi_i P(y^i | x^i ; \\theta).$$\n",
    "\n",
    "The equality here comes from the assumption that the labels of each observation $x^i$ are independent from one another. Since $y^i$ is Bernoulli with parameter $h(\\theta,x)$, setting $\\ell(\\theta) = \\log L(\\theta)$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\ell(\\theta) =& \\sum_i \\log \\left( h(\\theta , x^i)^{y^i}(1 - h(\\theta,x^i))^{1 - y^i} \\right) \\\\\n",
    "             =& \\sum_i y^i \\log h(\\theta, x^i) + (1 - y^i) \\log (1 - h(\\theta,x^i))\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "We seek to find the maximum of of $\\ell(\\theta)$. We first prove that this is a well-posed problem, i.e. that $\\ell(\\theta)$ is a concave function, $\\nabla^2 \\ell \\leq 0$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99e81e44",
   "metadata": {},
   "source": [
    "## Proof of the concavity of the log likelihood\n",
    "\n",
    "First observe that \n",
    "\n",
    "$$1 - g(z) = \\frac{\\exp (-z)}{1 + \\exp (-z)}$$ \n",
    "\n",
    "and so \n",
    "\n",
    "$$g'(z) = \\frac{\\exp (-z)}{(1 + \\exp (-z))^2} = g(z)(1 - g(z)).$$\n",
    "\n",
    "Consequently, $\\partial_{\\theta_j} h(\\theta,x) = g'(x \\cdot \\theta)x_j$ and\n",
    "\n",
    "$$\\partial_{\\theta_j}\\ell = \\sum_i y^i (1 - h(\\theta,x^i))x^i_j - (1 - y^i) h(\\theta,x^i)x^i_j $$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\partial^2_{\\theta_j \\theta_k} \\ell =& \\sum_i - y^i g'(\\theta \\cdot x^i)x^i_jx^i_k - (1 - y^i)g'(\\theta \\cdot x^i)x^i_k x^i_j \\\\\n",
    "=& - \\sum_i g'(\\theta \\cdot x^i) x^i_jx^i_k\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Notice that $x^i \\otimes x^i = x^i (x^i)^T \\geq 0$. This together with $g'(z) > 0$ implies that the hessian of $\\ell$ is negative semi-definite. To summarize, we have \n",
    "\n",
    "$$ \\nabla \\ell(\\theta) = \\sum_i (y^i - h(\\theta, x^i))x^i $$\n",
    "\n",
    "$$\\nabla^2 \\ell(\\theta) = -\\sum_i \\frac{\\exp(- x^i \\cdot \\theta) }{(1 + \\exp (- x^i \\cdot \\theta ))^2} x^i \\otimes x^i $$\n",
    "\n",
    "We will actually consider the convex normalized function $\\mathcal{L}(\\theta) = -\\frac{1}{m} \\ell(\\theta)$ and run newton's method to find the zero of \n",
    "\n",
    "$$ \\nabla \\mathcal{L}(\\theta) = \\frac{1}{m} \\sum_i (h(\\theta, x^i) - y^i)x^i $$\n",
    "\n",
    "which has derivative equal to the hessian,\n",
    "\n",
    "$$\\nabla^2 \\mathcal{L}(\\theta) = \\frac{1}{m} \\sum_i \\frac{\\exp(- x^i \\cdot \\theta) }{(1 + \\exp (- x^i \\cdot \\theta ))^2} x^i \\otimes x^i $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4dc6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Define the log reg prediction function h(theta,X,y).\n",
    "\n",
    "def h(theta,X):\n",
    "    '''(m,) shape array whose ith entry is the scalar h(theta,x^i) where x^i is the ith row of X.\n",
    "    \n",
    "    :param theta: ndarray of shape (n,)\n",
    "    :param X: ndarray of shape (m,n)\n",
    "    '''\n",
    "    \n",
    "    return np.reciprocal(1 + np.exp(-1 * (X @ theta) ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd246e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## This code block defines the functions needed for the .fit method of the Logistic Regression class.\n",
    "\n",
    "def gradient(theta, X, y):\n",
    "    '''vectorizes the gradient of the normalized convex log likelihood.\n",
    "    :param theta: ndarray of shape (n,)\n",
    "    :param X: ndarray of shape (m,n)\n",
    "    :param y: ndarray of shape (m,)\n",
    "    '''\n",
    "    m , _ = X.shape    \n",
    "    return 1/m * (h(theta,X) - y).T @ X\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451369f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(theta, X):\n",
    "    '''Tensor implementation of the hessian of the log likelihood.\n",
    "    \n",
    "    :param theta: ndarray of shape (n,)\n",
    "    :param X: ndarray of shape (m,n)\n",
    "    '''\n",
    "    m , _ = X.shape\n",
    "    \n",
    "    ## X_outer is the tensor product of X with itself. I.e. it has components X_outer_{ijkl} = X_{ij}X_{kl}\n",
    "    X_outer = np.multiply.outer(X,X) \n",
    "     \n",
    "    ## This is the (m,) array of coefficients of the $x^i \\otimes x^i$ in the formula for the hessian.\n",
    "    \n",
    "    coeff = h(theta,X)*(1 - h(theta,X))\n",
    "    \n",
    "    ## Einsum takes the full tensor product which has $ijklm entry coeff_iX_{jk}X_{lm} and contracts setting k = m = i.\n",
    "    \n",
    "    return ( 1 / m ) * np.einsum('i,ijik->jk', coeff , X_outer)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8cd101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X,y):\n",
    "    '''Finds the value of theta achieving maximum likelihood via newton\\'s method. Returns the optimal value of theta\n",
    "    as well as the number of iterations of newton\\'s method required to find it.\n",
    "    \n",
    "    :param X: ndarray of shape (m,n)\n",
    "    :param y: ndarray of shape (m,)\n",
    "    '''\n",
    "    m , n = X.shape\n",
    "    \n",
    "    j=0\n",
    "    theta = np.zeros(n)\n",
    "    theta_old = theta + 1\n",
    "    \n",
    "    while np.linalg.norm(theta - theta_old) > 10 ** (-5):\n",
    "        \n",
    "        theta_old = theta\n",
    "        \n",
    "        theta_new = theta - np.linalg.inv(hessian(theta,X)) @ gradient(theta,X,y)\n",
    "        \n",
    "        theta = theta_new\n",
    "        j+= 1\n",
    "    return (theta , j)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b7bc253",
   "metadata": {},
   "source": [
    "### The following code block loads the data sets from PS1 and checks the fit line our class generates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563c27c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13524\\3769052338.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mp01b_logreg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "import util\n",
    "import linear_model\n",
    "from p01b_logreg import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "x_train , y_train = util.load_dataset('../data/ds1_train.csv' , add_intercept=True)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "theta = np.zeros(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c7a0419",
   "metadata": {},
   "source": [
    "# Gaussian Discriminant Analysis\n",
    "\n",
    "## The hypotheses \n",
    "\n",
    "GDA is a generative algorithm, meaning for a binary classification problem $x \\in \\mathbb{R}^n$ and $y \\in \\{0,1 \\}$, it seeks to learn $p(x | y)$ by pre-supposing that both $p( x | y = 1)$ and $p(x | y = 0)$ are have multivariate Gaussian distributions with possibly different means, but the same covariance matrix. Recall that this means that the conditional distributions are\n",
    "\n",
    "$$p(x | y = i) \\sim q(z,\\mu_i,\\Sigma) = \\frac{1}{(2\\pi)^{\\frac{n}{2}} |\\Sigma|^\\frac{1}{2}}\\exp \\left(-\\frac{1}{2} \\langle \\Sigma^{-1}(z-\\mu_i) , (z-\\mu_i) \\rangle \\right) \\, dz$$\n",
    "\n",
    "Here, $\\mu_i \\in \\mathbb{R}^n$ and the *covariance matrix* $\\Sigma$ is $n \\times n$, symmetric, and positive definite. We also suppose that $y$ is Bernoulli with $p(y = 1) = \\phi$. Hence the parameters we are free to tune are $\\xi = (\\mu_0,\\mu_1, \\Sigma, \\phi)$.\n",
    "\n",
    "Recall that given random variables $X_1 , \\dots , X_n$ we define the *likelihood* of the observations $X_i = x_i$ is \n",
    "$L = p(x_1,\\dots,x_n)$ where $p$ is the joint distribution of the $X_i$. (Side note -- this only makes sense when the joint density can be interpreted as a function.)\n",
    "\n",
    "\n",
    "let $X^i$ be the $i^{th}$ observation of the data. In this case, since each data point is independent from one another, the random variables $Z_i = (X^i , y^i)$ which are valued in $\\mathbb{R}^n \\times \\{0,1\\}$, are independent. The density of each $Z_i$ is \n",
    "\n",
    "$$\\zeta_i(x,j) = p(x | y = j)p(j) = \\frac{1}{(2\\pi)^{\\frac{n}{2}} |\\Sigma|^\\frac{1}{2}}\\exp \\left(-\\frac{1}{2} \\langle \\Sigma^{-1}(x-\\mu_j) , (x-\\mu_j) \\rangle \\right) \\phi^{j}(1-\\phi)^{1-j} \\, dz$$\n",
    "\n",
    "And since we are assuming independence of the $Z_i$, the joint likelihood is equal to $L(\\theta) = \\Pi_i \\, \\zeta_i(x^i,y^i)$ and the log likelihood is\n",
    "\n",
    "$$\\ell(\\theta) = -\\sum_i \\frac{1}{2}\\left( n\\log 2\\pi + \\log \\det \\, \\Sigma + \\langle \\Sigma^{-1}(x^i - \\mu_{y^i}, x^i - \\mu_{y^i} \\rangle  \\right) - y^i\\log \\phi - (1-y^i)\\log(1-\\phi)$$\n",
    "\n",
    "after finding the parameters which maximize the joint likelihood, we predict $p(y | x)$ using **Bayes' rule:**\n",
    "\n",
    "$$p(y | X = x) = \\frac{p(x | Y = y) p_Y(y)}{p_X(x)} = \\frac{p(x | Y = y) p_Y(y)}{p(x | Y = 0)p_Y(0) + p(x | Y = 1)p_Y(1)}$$\n",
    "\n",
    "Here, $p_X$ and $p_Y$ are the distributions of $X$ and $Y$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf542b86",
   "metadata": {},
   "source": [
    "## Show that the GDA fit line is logistic\n",
    "\n",
    "We are going to show that there exists $\\theta \\in \\mathbb{R}^n$ and $\\theta_0 \\in \\mathbb{R}$ so that if $p(x|y = i)$ are distributed as above, then\n",
    "\n",
    "$$p(y = 1 | x ; \\, \\xi) = g(\\theta_0 + \\theta \\cdot x)$$\n",
    "\n",
    "where $g(z)$ is the logistic function. To put this differently, we are trying to show that $f( p(y = 1 | x ; \\, \\xi) )$ is affine in $x$, where $f = g^{-1}$. Now if \n",
    "\n",
    "$$y = \\frac{1}{1 + \\exp (-z)}$$\n",
    "\n",
    "then \n",
    "\n",
    "$$g^{-1}(y) = z = \\log \\frac{y}{1 - y}.$$\n",
    "\n",
    "Therefore, $$g^{-1}\\left(p(y = 1 | x)\\right) = \\Xi(x , \\mu , \\Sigma, \\phi) = \\log \\left( \\frac{p( y = 1 | X = x ; \\xi)}{p(y = 0 | X = x ; \\xi)}  \\right) $$\n",
    "\n",
    "Using Bayes' rule, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\Xi &= \\log \\left( \\frac{p(x | y = 1)p(y = 1)}{p(x | y = 0)p(y = 0)} \\right) \\\\\n",
    "    &= \\log \\left( \\frac{p(x | y = 1)}{p(x | y = 0)} \\right) + \\log \\frac{\\phi}{1 - \\phi}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "After writing out the Gaussian densities and taking logs we are left with one extra piece that goes into the $\\theta_0$ term (the normalizing coefficients) and a term which is quadratic in $x$. However, degree 2 part of the expression is the same in both the numerator and the denominator, so it cancels out and we are left with an affine expression in $x$. It is straightforward to check that this affine function is\n",
    "\n",
    "$$ \\Sigma(\\mu_1 - \\mu_0) \\cdot x + \\langle \\Sigma\\mu_0, \\mu_0 \\rangle - \\langle \\Sigma \\mu_1 , \\mu_1 \\rangle + \\log \\frac{\\phi}{1-\\phi}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b6c6eac",
   "metadata": {},
   "source": [
    "## Finding the parameters which maximize the joint likelihood of the GDA model\n",
    "\n",
    "We use the notation $\\xi = (\\phi,\\mu_0,\\mu_1, \\Sigma)$ to denote the full set of parameters we are maximizing with respect to. As shown above, the log likelihood is \n",
    "\n",
    "\n",
    "$$\\ell(\\theta) = - \\sum_i \\frac{1}{2} \\log \\det\\Sigma + \\frac{1}{2}\\Sigma^{-1}(x^i - \\mu^i) \\cdot (x^i - \\mu^i) - y^i\\log\\phi - (1 - y^i)\\log (1 - \\phi) $$\n",
    "\n",
    "The $\\mu^i$ appearing in the equation above is $\\mu^i = y^i\\mu_1 + (1 - y^i)\\mu_0$ and allows us to avoid splitting the sum according to the value of $y^i$. We have also ignored the constant term since our purpose is to differentiate the above formula. We now calculate where $\\ell$ is critical with respect to the full set of parameters $\\xi$.\n",
    "\n",
    "\n",
    "\n",
    "### The $\\phi$-critical equation.\n",
    "\n",
    "The first part of the sum has no $\\phi$ dependence, so differentiating we get\n",
    "\n",
    "$$\\partial_\\phi \\ell = \\sum_i \\frac{y^i}{\\phi} + \\frac{y^i- 1}{1 - \\phi} = \\sum_i \\frac{y^i - \\phi}{\\phi(1-\\phi)}$$\n",
    "\n",
    "This is zero if and only if $\\sum_i y^i = m \\phi$, $m$ being the total number of data points. Hence we see that \n",
    "\n",
    "$$\\phi = \\frac{ |\\{i : y^i = 1\\}| }{m}$$\n",
    "\n",
    "or in other words, $\\phi$ is the empirical chance of observing $y = 1$.\n",
    "\n",
    "### The $\\mu$-critical equations.\n",
    "\n",
    "The critical equations are, For $k \\in \\{0,1\\}$:\n",
    "\n",
    "$$ \\nabla_{\\mu_k}\\ell = \\sum_{\\{i: y^i = k \\}} \\Sigma^{-1}(x^i - \\mu_k) = 0$$\n",
    "\n",
    "Since $\\Sigma$ is non-singular, we must have\n",
    "\n",
    "$$\\frac{1}{|\\{i : y^i = k\\}|}\\sum_{\\{i: y^i = k\\}} x^i = \\mu_k$$\n",
    "\n",
    "i.e. each mean is the empirical average.\n",
    "\n",
    "### The $\\Sigma$-critical equation.\n",
    "\n",
    "We begin by noting that the space of positive definite symmetric matrices is an open subset of the vector space $\\mathscr{S}_n(\\mathbb{R})$ of all of $n \\times n$ real symmetric matrices. This means the tangent space at $\\Sigma$ can be identified with $\\mathscr{S}_n(\\mathbb{R})$. We will calculate $D\\ell_{\\Sigma}(X)$ for a given $X \\in \\mathscr{S}_n(\\mathbb{R})$. First we state two important formulas.\n",
    "\n",
    "Let $\\iota : GL_n(\\mathbb{R}) \\to GL_n(\\mathbb{R})$ be inversion, i.e. $\\iota(A) = A^{-1}$. Then, for any $n \\times n$ matrix $M$ (the tangent space to $GL_n(\\mathbb{R})$),\n",
    "\n",
    "$$D\\iota_A(M) = -A^{-1}MA^{-1}.$$\n",
    "\n",
    "Also, if $A$ positive definite and $F(A) = \\log \\det A$, then\n",
    "\n",
    "$$DF_A(X) = \\text{tr}\\, (A^{-T}X).$$\n",
    "\n",
    "If we write $v^i = x^i - \\mu^i$ then a straightforward calculation shows that $\\ell$ is critical at $\\Sigma$ if and only if for every $X \\in \\mathscr{S}_n(\\mathbb{R})$,\n",
    "\n",
    "$$ \\text{tr}\\,(X\\Sigma^{-1}) - \\frac{1}{m}\\sum_i \\langle X\\Sigma^{-1} v^i , \\Sigma^{-1} v^i \\rangle = 0.$$\n",
    "\n",
    "Note that for any $B \\in \\mathscr{S}_n(\\mathbb{R})$ and $y \\in \\mathbb{R}$, \n",
    "\n",
    "$$ \\text{tr}\\, (B yy^T) = B_ik (yy^T)_{ki} = B_{ik}y_ky_i = \\langle By , y\\rangle$$\n",
    "\n",
    "which means the critical equation can be re-written as\n",
    "\n",
    "\n",
    "$$\\text{tr} \\, (X\\Sigma^{-1}) - \\frac{1}{m} \\sum_i \\text{tr} \\, (X \\Sigma^{-1}v^i (v^i)^T \\Sigma^{-1}) = 0$$\n",
    "\n",
    "and by linearity of the trace, this means that \n",
    "\n",
    "$$X\\left( \\Sigma^{-1} - \\frac{1}{m} \\sum_i \\Sigma^{-1}v^i (v^i)^T \\Sigma^{-1}  \\right)$$\n",
    "\n",
    "is traceless for every $X \\in \\mathscr{S}_n(\\mathbb{R})$. But because $ (A,B) = \\text{tr}\\,(AB)$ is an inner product on symmetric matrices, this means \n",
    "\n",
    "$$\\Sigma^{-1} - \\frac{1}{m} \\sum_i \\Sigma^{-1}v^i (v^i)^T \\Sigma^{-1} = 0$$\n",
    "\n",
    "or $$\\Sigma = \\frac{1}{m}\\sum_i v^i (v^i)^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "359dd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code block defines the functions needed to fit the GDA parameters to a given dataset.\n",
    "\n",
    "def phi(X,y):\n",
    "    '''returns Bernoulli parameter phi that models the distribution of the labels y^i.\n",
    "    \n",
    "    :param X: array of shape (m,n). ith row should be ith data point x^i.\n",
    "    :param y: array of shape (m,). ith entry should be the label y^i of data point x^i.   \n",
    "    '''\n",
    "    m , _ = np.shape(X)\n",
    "    \n",
    "    return np.sum(y) / m\n",
    "\n",
    "def mu(X,y,i):\n",
    "    '''returns the conditional mean of X given y in the GDA model.\n",
    "    \n",
    "    :param X: array of shape (m,n).\n",
    "    :param y: array of shape (m,).\n",
    "    :param i: an integer 0 or 1 corresponding to a label.\n",
    "    '''\n",
    "    \n",
    "    m , _ = np.shape(X)\n",
    "    \n",
    "    m_1 = np.sum(y) \n",
    "    m_0 = m - m_1\n",
    "    \n",
    "    M = [m_0,m_1]\n",
    "    \n",
    "    \n",
    "    return 1 / M[i] * np.einsum('j,jk->k', i*y + (1 - i)*(1-y), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "727b2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(X,y):\n",
    "    '''returns the covariance matrix for the distributions of X given y.\n",
    "    \n",
    "    :param X: array of shape (m,n).\n",
    "    :param y: array of shape (m,).\n",
    "    '''\n",
    "    \n",
    "    m , _ = np.shape(X)\n",
    "    \n",
    "    ## An array whose rows are x^i - mu_i\n",
    "    \n",
    "    X_centered = np.einsum('i,ij->ij', 1-y , X - mu(X,y,0)) + np.einsum('i,ij->ij', y , X - mu(X,y,1))\n",
    "    \n",
    "    return (1/m) * np.einsum('ij,ik->jk', X_centered , X_centered )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1f3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(x,i):\n",
    "    '''\n",
    "    Vectorized implementation of the conditional distribution of x given y = i.\n",
    "    \n",
    "    Args:\n",
    "        x: An array of size (m,n)\n",
    "        i: An integer either 0 or 1.\n",
    "\n",
    "    Output: An array of size (m,) whose jth entry is the conditional density of X given y = i at the point x^j, the\n",
    "    j^th row of x.    \n",
    "    '''\n",
    "    \n",
    "    m , n = np.shape(x)\n",
    "    \n",
    "    d = np.linalg.det(sigma)\n",
    "    x_centered = x - mu[i]\n",
    "    \n",
    "    x_squared = np.tensordot(x_centered, x_centered, axes=0)\n",
    "    \n",
    "    ## Q is an array of shape (m,) whose ith entry is the quadratic form evaluated at the ith row of x.\n",
    "    \n",
    "    Q = - (1/2) * np.einsum('jk,ijik->i', sigma, x_squared)\n",
    "    \n",
    "    return (2* np.pi) ** (-n/2) * d ** (-1/2) * np.exp(Q) \n",
    "    \n",
    "\n",
    "def predict(x):\n",
    "    \n",
    "    prob = (p(x,1)*phi) / (p(x,1)*phi + p(x,0)*(1-phi))\n",
    "    \n",
    "    return (prob > 0.5).astype(np.int8)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "01e3a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Logistic_Regression_Class import LogisticRegression\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
