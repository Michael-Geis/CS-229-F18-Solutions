{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A)\n",
    "#### Show that the Poisson distribution is in the exponential family\n",
    "\n",
    "Recall that that the Poisson distribution with parameter $\\lambda$ is the discrete distribution \n",
    "\n",
    "$$p(y,\\lambda) = \\sum_{y=0}^\\infty e^{-\\lambda}\\frac{\\lambda^y}{y!}.$$\n",
    "\n",
    "A distribution is in the exponential family with parameter $\\eta$ if it can be written\n",
    "\n",
    "$$q(y,\\eta) = b(y)\\exp \\left(\\eta^T T(y) - a(\\eta) \\right)$$\n",
    "\n",
    "notice that the Poisson distribution can be written\n",
    "\n",
    "$$p(y,\\lambda) = \\frac{1}{y!} \\exp \\left( y \\log \\lambda - \\lambda \\right).$$\n",
    "\n",
    "therefore, $T(y) = y$, $\\eta = \\log \\lambda$, $a(\\eta) = e^\\eta$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B)\n",
    "#### What is the canonical response function for this family?\n",
    "\n",
    "The canonical response function is $g(\\eta) = E[T(y);\\eta]$. In this case $T(y) = y$, so\n",
    "\n",
    "$$g(\\eta) = E[p(y,\\lambda)] = \\sum_{y=1}^\\infty e^{-\\lambda} \\frac{\\lambda^y}{(y-1)!} = \\lambda = e^{\\eta}.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C)\n",
    "#### Derive the gradient ascent update rule for the negative conditional log likelihood of this GLM\n",
    "\n",
    "Let $\\eta^{(i)} = x^{(i)}\\cdot\\theta$ and assume that $y^{(i)} | x^{(i)} \\sim q(y^{(i)}, \\eta^{(i)})$. The negative log likelihood is\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=0}^m \\log q(y^{(i),\\eta^{(i)}}) = -\\frac{1}{m}\\sum_{i=0}^m -\\log y^{(i)}! + y^{(i)}(x^{(i)}\\cdot\\theta) - e^{x^{(i)}\\cdot\\theta} $$\n",
    "\n",
    "therefore the (batch) gradient is\n",
    "\n",
    "$$\\nabla J = -\\frac{1}{m}\\sum_{i=0}^m\\left(y^{(i)} - e^{x^{(i)} \\cdot \\theta} \\right)x^{(i)}$$\n",
    "\n",
    "so the stochastic gradient ascent update rule at step $i$ with learning rate $\\alpha$ is \n",
    "\n",
    "$$\\theta \\mapsto \\theta - \\alpha (y^{(i)} - e^{x^{(i)}\\cdot\\theta})x^{(i)}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
