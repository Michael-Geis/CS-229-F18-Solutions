{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of the KL divergence\n",
    "\n",
    "Given two discrete probability distributions $P$ and $Q$ over the outcome space $\\chi$, define their KL divergence by\n",
    "\n",
    "$$D_{KL}(P||Q) = \\sum_{x \\in \\chi} P(x)\\log \\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "## $D_{KL} \\geq 0$ and $D_{KL} = 0$ iff $P = Q$.\n",
    "\n",
    "We can re-write KL divergence as an expectation. With $f(x) = x\\log x$,\n",
    "\n",
    "$$D_{KL}(P||Q) = \\sum_{x \\in \\chi} Q(x) \\frac{P(x)}{Q(x)}\\log \\frac{P(x)}{Q(x)} = E_{x \\sim Q} \\, f\\left(\\frac{P(x)}{Q(x)}\\right)$$\n",
    "\n",
    "Since $f$ is strictly convex, we have Jensen's inequality $Ef(X)\\geq f(EX)$, so the RHS is bounded below by\n",
    "\n",
    "$$f \\left( E_{x \\sim Q} \\frac{P(x)}{Q(x)} \\right) = f(1) = 0$$\n",
    "\n",
    "Moreover if equality holds, then there is a constant $c$ so that $P(x)/Q(x) = c$ for all $x$ with $Q(x) > 0$. But this means that $P(x) = Q(x)$ for all such $x$ after summing both sides over $\\chi$. Then just note that this implies\n",
    "\n",
    "$$\\sum_{x: Q(x) = 0} P(x) = 1 - \\sum_{x : Q(x) > 0} P(x) = 0 $$\n",
    " \n",
    "which means $P = Q$ everywhere.\n",
    "\n",
    "\n",
    "## The 'Chain rule' \n",
    "\n",
    "### $$D_{KL}(P(X,Y)||Q(X,Y)) = D_{KL}(P(X|Y)||Q(X|Y)) + D_{KL}(P||Q)$$\n",
    "\n",
    "This follows in a completely straightforward way from writing out the definition. On the RHS, the conditional KL divergence is not exactly what \n",
    "one would naively think. It is defined by:\n",
    "\n",
    "$$D_{KL}(P(X|Y)||Q(X|Y)) = \\sum_y p(y) \\sum_x p(x|y) \\log \\frac{p(x|y)}{q(x|y)}$$\n",
    "\n",
    "That is, it is the expectation wrt to Y of the KL distance of the conditional distributions $P(X|Y)$ and $Q(X|Y)$\n",
    "\n",
    "## KL divergence and MLE\n",
    "\n",
    "Given a training set $S =\\{x^{(i)}\\}_{i=1}^m$, define the empirical distribution wrt this training set by \n",
    "\n",
    "$$P_S(x) = \\frac{|i | \\{x^{(i)} = x \\}|}{m}$$\n",
    "\n",
    "Given a family of probability distributions $P_\\theta \\sim p(x; \\theta) \\, dx$, we can calculate the KL divergence between the empirical measure and $P_\\theta$ by\n",
    "\n",
    "$$D_{KL}(P_S , P_\\theta) = -\\sum_{i=1}^m \\frac{1}{m} \\log m - \\frac{1}{m}\\sum_{i=1}^m \\log p(x^{(i)};\\theta)$$\n",
    "\n",
    "so the minimizing the KL divergence is equivalent to maximizing the log likelihood of $P_\\theta$.\n",
    "\n",
    "## An application of realizing MLE as minimizing $D_{KL}$\n",
    "\n",
    "The main point is it can be useful to apply the chain rule formula to simplify an optimizing problem. \n",
    "\n",
    "### Example: Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
