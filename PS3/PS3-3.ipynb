{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS3-3: KL Divergence, Fisher Information, and the Natural Gradient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "#### Dependence of gradient ascent on parametrization\n",
    "\n",
    "Suppose we have a family of probability distributions $P_\\theta = p(x;\\theta) \\, dx$ parametrized by $\\theta \\in \\mathbb{R}^{k}$. Suppose we are trying to find the distribution which maximizes some objective $J(P_\\theta)$. So far, we have considered gradient ascent with respect to $\\theta$. That is, after each iteration we update theta by the rule\n",
    "\n",
    "$$\\theta \\mapsto \\theta + \\alpha \\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "In a sense, the step size we are moving by is controlled by this parameter $\\alpha$, but the problem with this notion of distance per step is that it depends on the choice of parameterization $\\theta \\mapsto P_\\theta$. More precisely, the norm of the gradient $\\nabla_\\theta J(\\theta)$ will depend on the choice of parametrization, so the 'distance' towards the optimal distribution one moves in one update varies depending on the choice of parametrization. This notebook describes an alternate approach to finding the optimal distribution in which each update moves the distribution a fixed parameterization-independent 'distance' towards the optimal distribution. In this notebook the invariant notion of distance we will use is the KL divergence.\n",
    "\n",
    "## (A): The score function\n",
    "\n",
    "Given a parametrized family of distributions $p(x;\\theta)$, define the score function $S(x,\\theta)$ by\n",
    "\n",
    "$$S(x;\\theta) = \\nabla_\\theta \\log p(x;\\theta)$$\n",
    "\n",
    "If we were sampling $x$ from the distribution $P_\\theta$, the score function would be a random variable with its own distribution. This is a reasonable thing to study if we are interested in fitting via MLE, since we are trying to maximize the log likelihood $\\sum \\log p(x^{(i)};\\theta)$ assuming that the data $x^{(i)}$ is being sampled from $P_\\theta$ itself.\n",
    "\n",
    "#### Suppose that $x \\sim P_\\theta$. Prove that $E \\, S(x,\\theta) = 0$.\n",
    "\n",
    "Write out the RHS\n",
    "\n",
    "$$E S(x,\\theta) = \\int_{\\mathbb{R}^n} p(x;\\theta) \\nabla_\\omega \\log p(x,\\omega)|_{\\omega = \\theta} \\, dx = \\nabla_\\omega|_{\\omega =\\theta} \\int_{\\mathbb{R}^n} p(x,\\omega) \\, dx = 0$$\n",
    "\n",
    "## (B): Fisher information\n",
    "\n",
    "We now define the Fisher information $\\mathcal{I}(\\theta)$ to be the covariance of the score function:\n",
    "\n",
    "$$\\mathcal{I}(\\theta) = \\text{Cov}_{x \\sim P_\\theta} \\, S(x,\\theta) = E_{x \\sim P_\\theta} \\left( \\nabla_\\omega p(x;\\omega)|_{\\omega = \\theta}\\nabla_\\omega p(x;\\omega)|_{\\omega = \\theta}^T\\right)$$\n",
    "\n",
    "The formula in terms of expectation follows from the fact that $E[S(x,\\theta)] = 0$ from part (A).\n",
    "\n",
    "## (C): Alternate form of Fisher information\n",
    "\n",
    "For any function $f > 0$, we have the identity\n",
    "\n",
    "$$\\nabla^2 \\log f = f^{-1}\\nabla^2 f - (f^{-1}\\nabla f)(f^{-1}\\nabla f)^T$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$E_{x \\sim P_\\theta} -\\nabla^2_\\omega \\log p(x;\\omega)|_{\\omega = \\theta} = \\int_{\\mathbb{R}^n} -\\nabla^2_\\omega p(x;\\omega)|_{\\omega = \\theta} + p(x;\\theta) \\left(\\nabla_\\omega \\log p(x;\\omega)|_{\\omega = \\theta} \\right) \\left(\\nabla_\\omega \\log p(x;\\omega)|_{\\omega = \\theta} \\right)^T \\, dx$$\n",
    "\n",
    "The first term vanishes for the same reason as above and the second term is just the definition of covariance of the score function. Hence\n",
    "\n",
    "$$\\mathcal{I}(\\theta) = E_{x \\sim P_\\theta} -\\nabla^2_\\omega \\log p(x;\\omega) |_{\\omega = \\theta}$$\n",
    "\n",
    "## (D): Fisher information is the infinitisemal change in KL divergence\n",
    "\n",
    "Consider a parametrized family of probability distributions $P_\\theta$ with densities $p(x;\\theta) \\, dx$. For fixed $\\theta$, consider the increment in KL divergence:\n",
    "\n",
    "$$D(\\delta) = D_{KL}(P_{\\theta}||P_{\\theta + \\delta}) = \\int p(x;\\theta) \\log \\frac{p(x;\\theta)}{p(x;\\theta + \\delta)} \\, dx = \\int -p(x;\\theta)\\log p(x;\\theta + \\delta) \\, dx + \\text{const.}$$ \n",
    "\n",
    "It follows from the previous parts of this question that $D(0) = \\nabla_\\delta D |_{\\delta = 0} = 0$, and $\\nabla^2_\\delta D|_{\\delta = 0} = \\mathcal{I}(\\theta)$. Hence, by Taylor's formula, we have\n",
    "\n",
    "$$D_{KL}(P_\\theta||P_{\\theta + \\delta}) = \\frac{1}{2}\\bigg\\langle \\mathcal{I}(\\theta) \\delta , \\delta  \\bigg\\rangle + O(|\\delta|^3)$$\n",
    "\n",
    "## (E): The natural gradient update rule\n",
    "\n",
    "Let $\\ell(\\theta) = \\log p(x;\\theta)$ be the log-likelihood. Now we fix a small 'step size' $c$ and we seek to increase $\\ell(\\theta)$ as much as possible by updating $\\theta \\mapsto \\theta + \\delta$ while ensuring that the KL divergence $D_{KL}(P_\\theta||P_{\\theta+\\delta}) = c$. To do this we maximize $\\ell(\\theta + \\delta)$ subject to this constraint using Lagrange multipliers.\n",
    "\n",
    "We are looking for $\\delta$ such that there is a $\\lambda$ satisfying \n",
    "\n",
    "$$\\nabla_\\delta \\ell(\\theta + \\delta) = \\lambda \\nabla_\\delta D_{KL}(P_\\theta||P_{\\theta + \\delta})$$\n",
    "\n",
    "Replacing both sides by their taylor expansions at $\\delta = 0$, this is approximately\n",
    "\n",
    "$$\\nabla_\\omega \\ell(\\omega)|_{\\omega = \\theta} = \\lambda\\mathcal{I}(\\theta)\\delta$$\n",
    "\n",
    "or $$\\delta = \\frac{1}{\\lambda} \\mathcal{I}(\\theta)^{-1} \\nabla \\ell(\\theta)$$\n",
    "\n",
    "To determine $\\lambda$, use the constraint equation: \n",
    "\n",
    "$$D_{KL}(P_\\theta||P_{\\theta + \\delta}) \\simeq \\frac{1}{2}\\bigg\\langle \\mathcal{I}(\\theta)\\delta , \\delta \\bigg\\rangle = c$$\n",
    "\n",
    "which results in $\\lambda^2 = \\frac{1}{2c}\\langle \\mathcal{I}(\\theta) \\nabla \\ell(\\theta) , \\nabla \\ell(\\theta) \\rangle$. Finally note that $\\lambda > 0$ because of the positive definiteness of $\\mathcal{I}$; since $\\langle \\mathcal{I}^{-1}\\nabla \\ell , \\nabla \\ell \\rangle > 0$, $\\mathcal{I}^{-1}\\nabla \\ell$ points in the same direction as the gradient $\\ell$, which is the direction of increase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (F): Natural gradient ascent for GLMs\n",
    "\n",
    "We consider the example of fitting a general linear model. Recall that the exponential distributions depending on parameter $\\eta \\in \\mathbb{R}$ are given by\n",
    "\n",
    "$$p(y,\\eta) = b(y) \\exp \\left(\\eta y - a(\\eta) \\right)$$\n",
    "\n",
    "The hessian of the log-likelihood with respect to $\\eta$ is simply $-a''(\\eta)$ which does not depend on $y$, so it equals its expectation wrt $p(y,\\eta)$ This means the update direction of natural gradient ascent is \n",
    "\n",
    "$$-\\frac{1}{\\lambda}\\left(\\nabla_\\eta^2 \\log p(y,\\eta)\\right)^{-1} \\nabla_\\eta \\log p(y,\\eta)$$\n",
    "\n",
    "which differs from the update direction in newton's method applied to $-\\nabla_\\eta \\log p(y,\\eta)$:\n",
    "\n",
    "$$\\eta \\mapsto \\eta - (\\nabla_\\eta^2\\log p(y,\\eta))^{-1}\\nabla_\\eta \\log p(y,\\eta)$$\n",
    "\n",
    "by a positive scalar multiple."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
